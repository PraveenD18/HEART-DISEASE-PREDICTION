# -*- coding: utf-8 -*-
"""HDP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i8nA1bdVRnc3fOlZ_7-dZUanBmGD_BBU

## **Heart disease prediction using machine learning techniques**

**DATASET DESCRIPTION :**

age: The person's age in years

sex: The person's sex (1 = male, 0 = female)

cp: The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)

trestbps: The person's resting blood pressure (mm Hg on admission to the hospital)

chol: The person's cholesterol measurement in mg/dl

fbs: The person's fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)

restecg: Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)

thalach: The person's maximum heart rate achieved

exang: Exercise induced angina (1 = yes; 0 = no)

oldpeak: ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot. See more here)

slope: the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)

ca: The number of major vessels (0-3)

thal: A blood disorder called thalassemia (3 = normal; 6 = fixed defect; 7 = reversable defect)

target: Heart disease (0 = no, 1 = yes)
"""

# Importing the Dataset
import pandas as pd
df = pd.read_csv('/content/heart.csv')
df

df.shape

df.describe()

df.isnull().sum()

print(df.corr()["target"].abs().sort_values(ascending=False))

"""## **Expolatory Data Analysis**"""

import matplotlib.pyplot as plt
import seaborn as sns
Y = df['target'] # Output Feature

ax = sns.countplot(df["target"])
target_temp = df.target.value_counts()
print(target_temp)

print("Percentage of patience without heart problems : {}%".format(str(round(target_temp[0]*100/1025,2))))
print("Percentage of patience with heart problems : {}%".format(str(round(target_temp[1]*100/1025,2))))

ax = sns.countplot(df["sex"])
target_temp = df.sex.value_counts()
print(target_temp)

countFemale = len(df[df.sex == 0])
countMale = len(df[df.sex == 1])
print("Percentage of Female Patients : {:.2f}%".format((countFemale)/(len(df.sex))*100))
print("Percentage of Male Patients : {:.2f}%".format((countMale)/(len(df.sex))*100))

pd.crosstab(df.age,df.target).plot(kind="bar",figsize=(15,6))
plt.title('Heart Disease Frequency for Ages')
plt.xlabel('Age')
plt.ylabel('Target')
plt.savefig('heartDiseaseAndAges.png')
plt.show()

pd.crosstab(df.sex,df.target).plot(kind="bar",figsize=(15,10),color=['blue','#AA1111' ])
plt.title('Heart Disease Frequency for Sex')
plt.xlabel('Sex (0 = Female, 1 = Male)')
plt.xticks(rotation=0)
plt.legend(["Don't have Disease", "Have Disease"])
plt.ylabel('Target')
plt.show()

pd.crosstab(df.fbs,df.target).plot(kind="bar",figsize=(15,10),color=['#4286f4','#f49242'])
plt.title("Heart disease according to FBS")
plt.xlabel('FBS- (Fasting Blood Sugar > 120 mg/dl) (1 = true; 0 = false)')
plt.xticks(rotation=90)
plt.legend(["Don't Have Disease", "Have Disease"])
plt.ylabel('Target')
plt.show()

#Input Feature
X = df.iloc[:,:13]
X.head()

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X,Y,random_state=0)

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

from sklearn.preprocessing import StandardScaler

"""**Feature Scaling - Standardization**"""

st_x= StandardScaler()
x_train= st_x.fit_transform(x_train)
pd.DataFrame(x_train)

x_test = st_x.fit_transform(x_test)
x_test

"""# **Classification Models**"""

# Logistic Regression model
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(x_train,y_train)

y_pred1 = lr.predict(x_test)
pd.DataFrame({'Actual':y_test,'Predicted':y_pred1}).head(10)

from sklearn.metrics import confusion_matrix , ConfusionMatrixDisplay, accuracy_score
score_lr = accuracy_score(y_test,y_pred1)
cm = confusion_matrix(y_test, y_pred1)
cm_display = ConfusionMatrixDisplay(confusion_matrix = cm , display_labels = ['0','1'])
cm_display.plot()
plt.show()

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred1))

# Decision Tree model
from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier()
dtree.fit(x_train,y_train)

y_pred2 = dtree.predict(x_test)
pd.DataFrame({'Actual':y_test,'Predicted':y_pred2}).head(10)

from sklearn.metrics import confusion_matrix , ConfusionMatrixDisplay, accuracy_score
score_dt = accuracy_score(y_test,y_pred2)
cm = confusion_matrix(y_test, y_pred2)
cm_display = ConfusionMatrixDisplay(confusion_matrix = cm , display_labels = ['0','1'])
cm_display.plot()
plt.show()

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred2))

# K-NN model
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(x_train,y_train)

y_pred3 = knn.predict(x_test)
pd.DataFrame({'Actual':y_test,'Predicted':y_pred3}).head(10)

from sklearn.metrics import confusion_matrix , ConfusionMatrixDisplay, accuracy_score
score_knn = accuracy_score(y_test,y_pred3)
cm = confusion_matrix(y_test, y_pred3)
cm_display = ConfusionMatrixDisplay(confusion_matrix = cm , display_labels = ['0','1'])
cm_display.plot()
plt.show()

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred3))

"""For finding the no of estimators, we used the below code;

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
model=RandomForestClassifier()

params={'n_estimators':range(1,200)}

grid=GridSearchCV(estimator=model,cv=2,param_grid=params,scoring='neg_mean_squared_error')

grid.fit(x_train,y_train)

print("The best estimator returned by GridSearch CV is:",grid.best_estimator_)

**Output :**
The best estimator returned by GridSearch CV is: RandomForestClassifier(n_estimators=154)
"""

# Random Forest Classifier an Ensemble technique
from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators=60)
classifier.fit(x_train,y_train)

y_pred4 = classifier.predict(x_test)
pd.DataFrame({'Actual':y_test,'Predicted':y_pred4}).head(10)

from sklearn.metrics import confusion_matrix , ConfusionMatrixDisplay, accuracy_score
score_rf = accuracy_score(y_test,y_pred4)
cm = confusion_matrix(y_test, y_pred4)
cm_display = ConfusionMatrixDisplay(confusion_matrix = cm , display_labels = ['0','1'])
cm_display.plot()
plt.show()

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred4))

# Naive Baye's classifier
from sklearn.naive_bayes import GaussianNB
gb = GaussianNB()
gb.fit(x_train,y_train)

y_pred5 = gb.predict(x_test)
pd.DataFrame({'Actual':y_test,'Predicted':y_pred5}).head(10)

from sklearn.metrics import confusion_matrix , ConfusionMatrixDisplay, accuracy_score
score_nb = accuracy_score(y_test,y_pred5)
cm = confusion_matrix(y_test, y_pred5)
cm_display = ConfusionMatrixDisplay(confusion_matrix = cm , display_labels = ['0','1'])
cm_display.plot()
plt.show()

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred5))

# Ada Boost classifier
from sklearn.ensemble import AdaBoostClassifier
adbclassifier = AdaBoostClassifier()
adbclassifier.fit(x_train,y_train)

y_pred6 = adbclassifier.predict(x_test)
pd.DataFrame({'Actual':y_test,'Predicted':y_pred6}).head(10)

from sklearn.metrics import confusion_matrix , ConfusionMatrixDisplay, accuracy_score
score_adb = accuracy_score(y_test,y_pred6)
cm = confusion_matrix(y_test, y_pred6)
cm_display = ConfusionMatrixDisplay(confusion_matrix = cm , display_labels = ['0','1'])
cm_display.plot()
plt.show()

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred6))

"""For finding the no of estimators, we used the below code;

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingClassifier
model=GradientBoostingClassifier()

params={'n_estimators':range(1,200)}

grid=GridSearchCV(estimator=model,cv=2,param_grid=params,scoring='neg_mean_squared_error')

grid.fit(x_train,y_train)
print("The best estimator returned by GridSearch CV is:",grid.best_estimator_)

**OUTPUT** :

The best estimator returned by GridSearch CV is: GradientBoostingClassifier(n_estimators=164)
"""

# Gradient Boost classifier
from sklearn.ensemble import GradientBoostingClassifier
modelgf=GradientBoostingClassifier(n_estimators=164)
modelgf.fit(x_train,y_train)

y_pred7 = modelgf.predict(x_test)
pd.DataFrame({'Actual':y_test,'Predicted':y_pred7}).head(10)

from sklearn.metrics import confusion_matrix , ConfusionMatrixDisplay, accuracy_score
score_gb = accuracy_score(y_test,y_pred7)
cm = confusion_matrix(y_test, y_pred7)
cm_display = ConfusionMatrixDisplay(confusion_matrix = cm , display_labels = ['0','1'])
cm_display.plot()
plt.show()

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred7))

# Extreme Gradient boost classifier
from xgboost import XGBClassifier
xgb = XGBClassifier()
xgb.fit(x_train,y_train)

y_pred8 = xgb.predict(x_test)
pd.DataFrame({'Actual':y_test,'Predicted':y_pred8}).head(10)

from sklearn.metrics import confusion_matrix , ConfusionMatrixDisplay, accuracy_score
score_xgb = accuracy_score(y_test,y_pred8)
cm = confusion_matrix(y_test, y_pred8)
cm_display = ConfusionMatrixDisplay(confusion_matrix = cm , display_labels = ['0','1'])
cm_display.plot()
plt.show()

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred8))

#Neural Networks
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
from keras.layers import Dropout
from keras import regularizers


classifier = Sequential()
classifier.add(Dense(activation = "relu", input_dim = 13,
                     units = 8, kernel_initializer = "uniform"))
classifier.add(Dense(activation = "relu", units = 14,
                     kernel_initializer = "uniform"))
classifier.add(Dense(activation = "sigmoid", units = 1,
                     kernel_initializer = "uniform"))
classifier.compile(optimizer = 'adam' , loss = 'binary_crossentropy',
                   metrics = ['accuracy'] )

classifier.fit(x_train , y_train , batch_size = 8 ,epochs = 100  )

y_pred = classifier.predict(x_test)
y_pred = (y_pred > 0.5)

from sklearn.metrics import confusion_matrix , ConfusionMatrixDisplay, accuracy_score
score_nn = accuracy_score(y_test,y_pred)
cm = confusion_matrix(y_test, y_pred)
cm_display = ConfusionMatrixDisplay(confusion_matrix = cm , display_labels = ['0','1'])
cm_display.plot()
plt.show()

score_nn = (cm[0][0]+cm[1][1])/(cm[0][1] + cm[1][0] +cm[0][0] +cm[1][1])
print(score_nn*100)
print(classification_report(y_test,y_pred))



scores = [score_lr,score_dt,score_nb,score_knn,score_rf,score_nn]
algorithms = ["Logistic Regression","Decision Tree","Naive Bayes","K-Nearest Neighbors","Random Forest","Neural Networks"]

for i in range(len(algorithms)):
    print("The accuracy achieved using "+algorithms[i]+" is: "+str(round(scores[i]*100,2))+" %")

sns.set(rc={'figure.figsize':(15,8)})
plt.xlabel("Algorithms")
plt.ylabel("Accuracy score")

sns.barplot(algorithms,scores)

"""**Hence, Random Forest gives best accuracy (i.e. 100%) compared to all other models**

## **FEATURE EXTRACTION**
"""

feature_importance=pd.DataFrame(classifier.feature_importances_,index=X.columns,columns=['importance']).sort_values('importance')
feature_importance

feature_importance.sum()

"""Taking above 0.09;"""

x1 = df.drop(['fbs','restecg','sex','slope','exang','trestbps','chol'],axis=1)
x1

y1 = df['target']
y1

from sklearn.model_selection import train_test_split
x_train1,x_test1,y_train1,y_test1 = train_test_split(x1,y1,random_state=0)

from sklearn.preprocessing import StandardScaler
st_x = StandardScaler()
x_train1 = st_x.fit_transform(x_train1)
x_test1 = st_x.fit_transform(x_test1)

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(x_train1,y_train1)

y1_pred = lr.predict(x_test1)
from sklearn.metrics import confusion_matrix, classification_report
print(confusion_matrix(y_test1,y1_pred))
print(classification_report(y_test1,y1_pred))

from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier()
dtree.fit(x_train1,y_train1)

y2_pred = dtree.predict(x_test1)
from sklearn.metrics import confusion_matrix, classification_report
print(confusion_matrix(y_test1,y2_pred))
print(classification_report(y_test1,y2_pred))